{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy-based Neural Network engine from scratch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/m12sl/dl-hse-2020/blob/master/01-introduction/numpy%20neural%20networks%20from%20scratch.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "В этой тетрадке мы напишем собственный простенький фреймворк для создания и обучения нейронной сети.\n",
    "\n",
    "**Цели тетрадки**\n",
    "\n",
    "1. Понять, что лежит в основе DL фреймворков\n",
    "2. Разобраться с бекпропом на примере простых слоев\n",
    "3. Встретить характерные для векторизованных вычислений ошибки\n",
    "4. Вспомнить особенности операций с плавающей точкой\n",
    "5. Попробовать тестировать код сразу\n",
    "\n",
    "\n",
    "**План работы**\n",
    "\n",
    "0. Обсудить векторизацию вычислений и договориться о формате входных-выходных данных\n",
    "1. Рассмотреть как высокоуровнево устроен движок для сеток\n",
    "2. Реализовать несколько базовых слоев\n",
    "3. Натренировать простую сеть на задаче MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторизация вычислений (batchification)\n",
    "\n",
    "<img src=\"https://github.com/m12sl/dl-hse-2020/raw/master/01-introduction/img/graph_vector.png\" crossorigin=\"anonymous\"/>\n",
    "\n",
    "В современных компьютерах выгодно объединять данные к которым будет применена одна и та же операция в вектора и батчи. Для вычисления длин нескольких векторов их, как правило, выгодно склеить в массив (numpy.ndarray) и применить векторизованную операцию (все numpy-операции работают с многомерными массивами).\n",
    "\n",
    "\n",
    "На практике во всех фреймворках для DL (да и для ML) работают с данными упакованными в батчи: \n",
    "многомерные массивы с размерами `[batch_size, channels, *spatial_dimensions]` -- такие массивы называются тензорами. \n",
    "\n",
    "Подразумевается, что примеры в батче независимы друг от друга и никак не влияют друг на друга (за исключением некоторых нормализаций и специальных трюков).\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/m12sl/dl-hse-2020/raw/master/01-introduction/img/graph_batched.png\" crossorigin=\"anonymous\"/>\n",
    "\n",
    "\n",
    "Предлагается реализовать простой движок для обучения простых нейронных сетей (MLP, Multi Layer Perceptron).\n",
    "\n",
    "Класс для обходов вычислительного графа предлагается взять таким:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SomeNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grads):\n",
    "        for layer in self.layers[::-1]:\n",
    "            grads = layer.backward(grads)\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слои -- это объекты с двумя методами (`forward`, `backward`) и конструктором (`__init__`).\n",
    "\n",
    "**Для простоты предлагается вычисление градиентов по весам и их применение объединить с backward-проходом.**\n",
    "\n",
    "Операция `Wx + b` формально имеет три входа: входной тензор $x$ и веса: $W$ и $b$. Мы разместим веса в слое, так что слой на прямом проходе будет иметь только один вход `x`, а на обратно выдавать градиенты $\\frac{\\partial L}{\\partial x}$.\n",
    "\n",
    "\n",
    "**Посмотрите внимательно на прототип слоя, вам потребуется реализовать Linear и ReLU на основе него:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Это на самом деле identity\n",
    "class Layer:\n",
    "    def forward(self, input):\n",
    "        # store something if needed\n",
    "        return input\n",
    "    \n",
    "    def backward(self, grads):\n",
    "        # apply grads here if any\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сеть которая не делает ничего:\n",
    "network = SomeNetwork([Layer(), Layer()])\n",
    "\n",
    "x = np.random.random(size=(3, 5))\n",
    "output = network.forward(x)\n",
    "assert output.shape == (3, 5)\n",
    "\n",
    "y = np.ones((3, 5))\n",
    "grads = network.backward(y)\n",
    "assert grads.shape == (3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве сети возьмем самую простую: Multilayer Perceptron (MLP).\n",
    "\n",
    "<img src=\"https://github.com/m12sl/dl-hse-2020/raw/master/01-introduction/img/graph_mlp.png\" crossorigin=\"anonymous\"/>\n",
    "\n",
    "\n",
    "Нам потребуется реализовать следующие слои:\n",
    "\n",
    "```\n",
    "Linear\n",
    "Relu\n",
    "```\n",
    "\n",
    "Целевой задачей возьмем многоклассовую классификацию, мы хотим, чтобы наша сеть для каждого входного примера предсказывала распределение вероятностей $q_i$.\n",
    "\n",
    "Функция ошибки должна отражать близость распределения $q$ к известным правильным ответам. В этом нам поможет кросс-энтропийная функция ошибки (перекрестная энтропия):\n",
    "$$\n",
    "H(p, q) = - \\sum \\limits_{i} p_i \\log q_i \n",
    "$$\n",
    "\n",
    "Эта формула описывает близость между двумя распределениями вероятностей:\n",
    " - $i$ --- номер класса,\n",
    " - $p_i$ --- правильная вероятность класса (единица, еcли правильная метка i, ноль, если нет)\n",
    " - $q_i$ --- предсказанная вероятность (выход сети)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layer\n",
    "\n",
    "Синонимы: Linear, Dense, FullyConnected, Projection:\n",
    "\n",
    "Математическое действие над вектором признаков:\n",
    "$$\\mathbf{y} = W \\mathbf{x} + \\mathbf{b}$$\n",
    "\n",
    "Пусть \n",
    "$\\mathbf{x} \\in \\mathbb{R}^n$\n",
    "$\\mathbf{y}\\in \\mathbb{R}^m$\n",
    "\n",
    "**Какую размерность должен иметь $W$ и $b$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь реализуем векторизованный вариант слоя: `y = f(x, W, b)`\n",
    "\n",
    "```python\n",
    "x.shape == (batch_size, in_channels)\n",
    "y.shape == (batch_size, out_channels)\n",
    "```\n",
    "\n",
    "**0.5 балла** Напишите код для вычисления прямого прохода и обратного в векторизованной форме.\n",
    "Шаг градиентного спуска предлагается совместить с обратным проходом.\n",
    "\n",
    "Веса $W, b$ считаем принадлежащими слою, инициализируем при создании, во время обратного прохода возвращаем градиенты только за входной тензор.\n",
    "\n",
    "**NB: нас не интересует строгое соответствие математической нотации, она годится для работы с векторами. Мы же работаем с данными собранными в батчи.**\n",
    "\n",
    "Для простоты движка важно, чтобы операции получали и возвращали данные в правильном формате. \n",
    "В данном случае -- это numpy.ndarray с размерностями `[bs, dim]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуйте линейный слой\n",
    "# y = Wx + b\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_channels, out_channels, learning_rate=0.1):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.W = np.random.randn(in_channels, out_channels) * 0.01\n",
    "        self.b = np.zeros(out_channels)\n",
    "        self.learning_rate = learning_rate\n",
    "        # for debug purpose\n",
    "        self.grad_W = None\n",
    "        self.grad_b = None\n",
    "        self.grad = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        <напишите свой код>\n",
    "        return y\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        <напишите вычисление градиентов>\n",
    "        \n",
    "        self.W = self.W - grad_W * self.learning_rate\n",
    "        self.b = self.b - grad_b * self.learning_rate\n",
    "        \n",
    "        self.grad_W = grad_W\n",
    "        self.grad_b = grad_b\n",
    "        self.grad = grad\n",
    "        return grad\n",
    "\n",
    "    \n",
    "# тесты для самопроверки\n",
    "linear = Linear(5, 7)\n",
    "\n",
    "x = np.random.random(size=(3, 5))\n",
    "assert linear.forward(x).shape == (3, 7), \"Forward pass shape mistmatch\"\n",
    "\n",
    "y = np.random.random(size=(3, 7))\n",
    "grad_x = linear.backward(y)\n",
    "assert grad_x.shape == (3, 5)\n",
    "assert linear.grad_W.shape == linear.W.shape\n",
    "assert linear.grad_b.shape == linear.b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Layer\n",
    "\n",
    "Это простая поэлементная операция, записывается как $f(x) = max(x, 0)$.\n",
    "\n",
    "**NB: активационные слои не меняют размер тензора**\n",
    "\n",
    "**0.3 балла**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def forward(self, input):\n",
    "        <напишите код>\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        <напишите код>\n",
    "    \n",
    "relu = ReLU()\n",
    "x = np.random.normal(size=(3, 5))\n",
    "assert relu.forward(x).shape == (3, 5)\n",
    "assert np.all(relu.forward(x) >= 0.0)\n",
    "\n",
    "y = np.random.normal(size=(3, 5))\n",
    "grads = relu.backward(y)\n",
    "assert grads.shape == (3, 5) \n",
    "\n",
    "approx = (relu.forward(x + 1e-5) - relu.forward(x)) / 1e-5\n",
    "\n",
    "relu.forward(x)\n",
    "assert np.allclose(relu.backward(np.ones_like(x)), approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подсчета ошибки нам необходимы предсказания сети и правильные ответы.\n",
    "\n",
    "Для простоты подсчет ошибки и градиентов вынесены в отдельные функции вместе с вычислением вероятностей (softmax).\n",
    "\n",
    "Каждая из функций ожидает на вход логиты -- тензор с размерами `[bs, num_classes]`.\n",
    "\n",
    "`softmax_crossentropy_with_logits` возвращает одно число -- величину ошибки.\n",
    "\n",
    "`grad_softmax_crossentropy_with_logits` возвращает тензор размером `[bs, num_classes]` с градиентами по логитам.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits, reference_answers):\n",
    "    \"\"\"Compute crossentropy from logits[batch, n_classes] and ids of correct answers\"\"\"\n",
    "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
    "    \n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    \n",
    "    return xentropy.mean()\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits, reference_answers):\n",
    "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (- ones_for_answers + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handling\n",
    "Для работы с данными во всех фреймворках есть Dataset/Dataloader классы.\n",
    "\n",
    "Мы будем разбирать pytorch Data API отдельно, а пока предлагается воспользоваться готовым кодом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST, M\n",
    "\n",
    "# в FashionMNIST уже есть код, который грузит данные и читает их в PIL.Image, мы конвертируем их в вектора.\n",
    "# Непосредственно с картинками мы будем работать позже\n",
    "def transform(pil_image):\n",
    "    return np.asarray(pil_image).astype(np.float32).reshape(-1) / 255.0\n",
    "\n",
    "\n",
    "def numpy_collate_fn(batch):\n",
    "    x, y = list(zip(*batch))\n",
    "    return np.array(x).astype(np.float32), np.array(y).astype(np.int32)\n",
    "\n",
    "\n",
    "# dataset умеет выдавать отдельные семплы с метками\n",
    "train_dataset = MNIST(\"./tmp2\", train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(\"./tmp2\", train=False, download=True, transform=transform)\n",
    "# dataloader выдает целые батчи и поддерживает итерирование\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32, collate_fn=numpy_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=32, collate_fn=numpy_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset возвращает отдельные семплы\n",
    "print(train_dataset[11][0].shape, train_dataset[11][1])\n",
    "\n",
    "# dataloader целые батчи\n",
    "for x, y in train_loader:\n",
    "    print(x.dtype, y.dtype)\n",
    "    print(x.shape, y.shape)\n",
    "    break\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(f\"Class: {train_dataset[i][1]}\")\n",
    "    plt.imshow(train_dataset[i][0].reshape(28, 28),cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop\n",
    "\n",
    "Теперь все готово чтобы сделать сеть и проучить ее.\n",
    "Если все реализовано правильно, можно ожидать точности на валидации ~0.86+.\n",
    "\n",
    "Кросс-энтропийный лосс не слишком показательная величина, но если он должен оказаться ~0.4 на валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SomeNetwork([\n",
    "    Linear(784, 40),\n",
    "    ReLU(), \n",
    "    Linear(40, 40),\n",
    "    ReLU(),\n",
    "    Linear(40, 10),\n",
    "])\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for x, y in tqdm(train_loader):\n",
    "        logits = network.forward(x)\n",
    "        loss = softmax_crossentropy_with_logits(logits, y)\n",
    "        acc = np.mean(np.argmax(logits, axis=-1) == y)\n",
    "        grads = grad_softmax_crossentropy_with_logits(logits, y)\n",
    "        network.backward(grads)\n",
    "        \n",
    "    logs = defaultdict(list)\n",
    "    for x, y in tqdm(test_loader):\n",
    "        logits = network.forward(x)\n",
    "        loss = softmax_crossentropy_with_logits(logits, y)\n",
    "        acc = np.mean(np.argmax(logits, axis=-1) == y)\n",
    "        logs['loss'].append(loss)\n",
    "        logs['acc'].append(acc)\n",
    "        \n",
    "    for k, v in logs.items():\n",
    "        print(k, np.mean(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0.1 балл**. Постройте графики обучения: метрики от количества шагов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0.1 балл**. Выведите пример картинок, предсказаний и правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
