{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMT with Transformer&BPE\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/m12sl/dl-hse-2020/blob/master/08-nlp-part2/NMT-Transformers-bpe.ipynb)\n",
    "\n",
    "**Цели тетрадки**\n",
    "\n",
    "- Познакомитсья с BPE (будем пользоваться пакетом youtokentome)\n",
    "- Освоить работу с nn.Transformer\n",
    "- Натренировать модель для переводов en <--> ru\n",
    "\n",
    "\n",
    "**План**\n",
    "\n",
    "- Предобработать данные и обучить BPE-модель\n",
    "- Написать обвязку вокруг nn.Transformer для задачи перевода\n",
    "- Натренировать и проверить модель\n",
    "\n",
    "\n",
    "**Настоятельно рекомендуется воспользоваться колабом**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# установим отсутствующие библиотеки:\n",
    "! pip install youtokentome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMT с помощью трансформеров и BPE представлений.\n",
    "Начинаем точно так же с датасета:\n",
    "\n",
    "\n",
    "Для преобразования текстов в BPE и обратно воспользуемся библиотекой\n",
    "https://github.com/VKCOM/YouTokenToMe\n",
    "\n",
    "Она быстро работает, позволяет хранить данные в текстовом формате и преобразовывать в BPE (с BPE-дропаутом) прямо на лету."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import unicodedata\n",
    "\n",
    "import youtokentome as yttm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "Работать будем с тем же датасетом en+ru с сайта открытого проекта tatoeba.\n",
    "Но на этот раз в качестве будем представлять данные не в виде слов, а в виде sub-word частей, с помощью Byte Pair Encoding.\n",
    "\n",
    "Для работы с BPE возьмем библиотеку [YouTokenToMe](https://github.com/VKCOM/YouTokenToMe). \n",
    "\n",
    "Она работает быстро, это позволит нам хранить данные в текстовом формате, и делать все необходимые преобразования прямо на лету."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -qO- https://github.com/m12sl/dl_cshse_2019/raw/master/seminars/x2seq/eng-rus.tar.gz | tar xzvf -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приготовим данные и посмотрим на них\n",
    "# Кроме словаря нас интересует еще набор символов\n",
    "raw_alphabet = set()\n",
    "alphabet = set()\n",
    "def normalize(s):\n",
    "    return \"\".join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess(s):\n",
    "    raw_alphabet.update(s)\n",
    "    s = normalize(s.lower().strip())\n",
    "    s = re.sub(r\"[^a-zа-я?.,!]+\", \" \", s)\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    alphabet.update(s)\n",
    "    return s\n",
    "\n",
    "pairs = []\n",
    "with open('eng-rus.txt', 'r') as fin:\n",
    "    for line in tqdm(fin.readlines()):\n",
    "        pair = [preprocess(_) for _ in line.split('\\t')]\n",
    "        pairs.append(pair)\n",
    "\n",
    "print(\"RAW alphabet {} symbols:\".format(len(raw_alphabet)), \n",
    "      \"\".join(sorted(raw_alphabet)))\n",
    "print(\"After preprocessing {} symbols: \".format(len(alphabet)), \n",
    "      \"\".join(sorted(alphabet)))\n",
    "print(\"There are {} pairs\".format(len(pairs)))\n",
    "print(pairs[10101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## обучение BPE\n",
    "\n",
    "BPE позволяет нам проучить словарь произвольных размеров. \n",
    "\n",
    "Например, мы можем сделать общий словарь для английского и русского.\n",
    "Для этого надо записать все доступные тексты в один файл и проучить BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for p in pairs:\n",
    "    lines += p\n",
    "lines = list(set(lines))\n",
    "with open(\"./all.txt\", \"w\") as fout:\n",
    "    for line in lines:\n",
    "        fout.write(line + \"\\n\")\n",
    "\n",
    "! head all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = 5000\n",
    "bpe = yttm.BPE.train(data=\"./all.txt\", vocab_size=VOCAB, model=\"enru.bpe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB: можно ли учить BPE на всем датасете\n",
    "\n",
    "Во многих задачах возникает вопрос касательно расчета статистик на всем датасете:\n",
    "\n",
    "<mark>если _что-то_ является важным признаком, нужно это _что-то_ считать только по трейну, или можно взять весь датасет с валидацией?</mark>\n",
    "\n",
    "- можно ли считать средние по всем доступным данным для задачи прогнозирования временных рядов\n",
    "- можно ли считать word2vec на всем датасете\n",
    "- и т.д.\n",
    "\n",
    "Простого ответа нет, в данном случае BPE не является в прямом смысле моделью, но изменение статистик может влиять на состав словаря:\n",
    "\n",
    "<img src=\"https://github.com/m12sl/dl-hse-2020/raw/master/08-nlp-part2/img/bpe.gif\" crossorigin=\"anonymous\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe.encode(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bpe.encode(lines[:10], output_type=yttm.OutputType.ID))\n",
    "print(bpe.encode(lines[:10], output_type=yttm.OutputType.SUBWORD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE Dropout\n",
    "\n",
    "(см статью: [BPE-Dropout: Simple and Effective Subword Regularization](https://arxiv.org/abs/1910.13267))\n",
    "\n",
    "В очень больших BPE словарях (5к токенов на два языка -- это небольшой словарь) встречается проблема: некоторые токены есть в словаре, но не встречаются в трейне. \n",
    "\n",
    "Они могут встречаться в реальных данных, благодаря естественным процессам или опечаткам. Для борьбы с таким явлением и просто в качестве регуляризации можно применить BPE-dropout: случайное переразбиение строки на токены.\n",
    "<img src=\"https://github.com/m12sl/dl-hse-2020/raw/master/08-nlp-part2/img/bpe-dropout.png\" crossorigin=\"anonymous\"/>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bpe.encode(lines[:1], dropout_prob=0.0, output_type=yttm.OutputType.SUBWORD))\n",
    "print(bpe.encode(lines[:1], dropout_prob=0.2, output_type=yttm.OutputType.SUBWORD))\n",
    "print(bpe.encode(lines[:1], dropout_prob=0.5, output_type=yttm.OutputType.SUBWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded0 = [len(_) for _ in bpe.encode(lines, dropout_prob=0.0)]\n",
    "encoded1 = [len(_) for _ in bpe.encode(lines, dropout_prob=0.1)]\n",
    "encoded2 = [len(_) for _ in bpe.encode(lines, dropout_prob=0.5)]\n",
    "\n",
    "sns.distplot(encoded0, kde=False, label=\"no do\")\n",
    "sns.distplot(encoded1, kde=False, label=\"do 0.1\")\n",
    "sns.distplot(encoded2, kde=False, label=\"do 0.5\")\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# предлагается максимальную длину строки ограничить 100 токенами, а для обучения использовать BPE_DO=0.1\n",
    "MAX_LENGTH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Датасет в этот раз минимально простой: возвращает словарь с en и ru строками, без преобразований.\n",
    "\n",
    "collate_fn нам не потребуется, а конвертацию в BPE мы опишем внутри класса модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pairset:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        en, ru = self.data[item]\n",
    "        return dict(en=en, ru=ru)\n",
    "\n",
    "train_pairs, val_pairs = train_test_split(pairs, test_size=0.3)\n",
    "\n",
    "trainset = Pairset(train_pairs)\n",
    "valset = Pairset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "it = iter(trainloader)\n",
    "print(next(it))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop\n",
    "\n",
    "Тренировочный цикл обычный. Подразумевается, что у модели есть два метода:\n",
    "\n",
    "```\n",
    "model.compute_all(batch) -> Dict\n",
    "model.check_translations(batch) -> None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, opt, trainloader, valloader, epochs=1):\n",
    "    step = 0\n",
    "    logs = defaultdict(list)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in tqdm(trainloader):\n",
    "            details = model.compute_all(batch)\n",
    "            loss = details[\"loss\"]\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            step += 1\n",
    "            [logs[k].append(v) for k, v in details[\"metrics\"].items()]\n",
    "            \n",
    "        model.eval()\n",
    "        tmp = defaultdict(list)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(valloader):\n",
    "                details = model.compute_all(batch)\n",
    "                for k, v in details[\"metrics\"].items():\n",
    "                    tmp[k].append(v)\n",
    "            tmp = {k: np.mean(v) for k, v in tmp.items()}\n",
    "            [logs[f\"val_{k}\"].append(v) for k, v in tmp.items()]\n",
    "            logs[\"step\"].append(step)\n",
    "            model.check_translations(batch)\n",
    "        \n",
    "        for key in [\"loss\"]:\n",
    "            plt.figure()\n",
    "            plt.title(key)\n",
    "            plt.plot(logs[key], label=\"train\", c='b', zorder=1)\n",
    "            plt.scatter(logs[\"step\"], logs[f\"val_{key}\"], label=\"val\", c='r', zorder=10)\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи и Модели\n",
    "\n",
    "В прошлый раз мы делали RNN Encoder-Decoder модель, которая переводила с одного языка на другой.\n",
    "\n",
    "<img src=\"https://github.com/m12sl/dl-hse-2020/raw/master/08-nlp-part2/img/nmt-encoder-decoder.png\" crossorigin=\"anonymous\"/>\n",
    "\n",
    "Какие есть варианты, как еще можно сделать перевод?\n",
    "\n",
    "**NMT как Language Model (или как автодополнение)**\n",
    "\n",
    "Мы можем рассматривать перевод как задачу продолжения текста и учить как обычную одностороннюю языковую модель.\n",
    "\n",
    "<img src=\"https://github.com/m12sl/dl-hse-2020/raw/master/08-nlp-part2/img/nmt-lm.png\" crossorigin=\"anonymous\"/>\n",
    "\n",
    "\n",
    "Что, если мы хотим переводить с множества языков на множество других?\n",
    "\n",
    "Простейший вариант: тренировать отдельные модели на каждую пару языков.\n",
    "\n",
    "Более интересный вариант: для каждого языка тренировать энкодер в общее представление и декодер из него.\n",
    "\n",
    "И один из наиболее интересных вариантов: делать переводы в рамках одной модели.\n",
    "\n",
    "**NMT как Text-to-Text**\n",
    "\n",
    "Можно сказать модели какой перевод нужно сделать, например с помощью специального токена в энкодер:\n",
    "\n",
    "<img src=\"https://github.com/m12sl/dl-hse-2020/raw/master/08-nlp-part2/img/nmt-cmd-1.png\" crossorigin=\"anonymous\"/>\n",
    "\n",
    "Специальный токен можно подать в качестве первого токена в декодер:\n",
    "\n",
    "<img src=\"https://github.com/m12sl/dl-hse-2020/raw/master/08-nlp-part2/img/nmt-cmd-2.png\" crossorigin=\"anonymous\"/>\n",
    "\n",
    "\n",
    "\n",
    "Разумеется можно сделать иначе, сделать у сети отдельный вход для флажка, но подход с указанием задачи в тексте выглядит изящнее всего.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Model To Rule Them All\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b7/Unico_Anello.png\" crossorigin=\"anonymous\"/>\n",
    "\n",
    "\n",
    "Пободный подход используют в [модели T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html): модель учат решать разные задачи, задавая их токенами в SRC:\n",
    "<img src=\"https://miro.medium.com/max/4006/1*D0J1gNQf8vrrUpKeyD8wPA.png\" crossorigin=\"anonymous\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Transformer`\n",
    "\n",
    "Официальная документация по (nn.Transformer)[https://pytorch.org/docs/master/generated/torch.nn.Transformer.html#transformer] довольно скудная.\n",
    "\n",
    "Но важные моменты таковы:\n",
    "\n",
    "0. Входные и выходные данные надо готовить самостоятельно: понадобится написать positional и token embeddings, а также выходной FC-слой\n",
    "\n",
    "1. nn.Transformer.forward берет на себя прогон энкодера, и правильное применение декодера.\n",
    "\n",
    "2. Порядок осей такой же, как при использовании RNN-моделей (для совместимости в seq2seq задачах): `[seq_len, batch_size, dimension]`.\n",
    "\n",
    "3. Обязательно надо задавать `src_key_padding_mask` и `tgt_key_padding_mask` для маскирования недоступных для внимания токенов (в частности паддингов).\n",
    "\n",
    "\n",
    "\n",
    "Предлагается завести два спецтокена: для перевода в русский язык и в английский, с номерами `bpe.vocab_size()` и `bpe.vocab_size() + 1`. \n",
    "Эти токены можно не генерировать с помощью выходного слоя, но на входе они могут быть.\n",
    "\n",
    "\n",
    "\n",
    "Предлагается написать следующие функции:\n",
    "\n",
    "```\n",
    "model.encode(list_of_strings) # функция, которая переводит строку в последовательность номеров BPE-токенов, добавляет спецтокены и паддит до MAX_LENGTH\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "model.check_translations(batch) # функция, которая сделает и выведет перевод для батча с примерами\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "model.compute_all(batch) # функция для обучения, прогонит батч, посчитает лосс и вернет словарь с метриками и лоссом\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VeryT(nn.Module):\n",
    "    def __init__(self, bpe, bpe_dropout=0.1, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bpe = bpe\n",
    "        self.bpe_dropout = bpe_dropout\n",
    "        self.embeddings = nn.Embedding(bpe.vocab_size() + 2, hidden_size)\n",
    "        self.positional_embeddings = nn.Embedding(MAX_LENGTH, hidden_size)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=hidden_size, \n",
    "            nhead=8, \n",
    "            num_encoder_layers=3, \n",
    "            num_decoder_layers=3, dim_feedforward=512)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, bpe.vocab_size()),\n",
    "            nn.LogSoftmax(dim=-1),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def encode(self, lst, pre=None, post=None, seq_len=None, dropout=0.0):\n",
    "        lst = [self.bpe.encode(entry, dropout_prob=dropout) for entry in lst]\n",
    "        ## add tokens and paddings\n",
    "        <your code>\n",
    "        return lst\n",
    "        \n",
    "    def check_translations(self, batch):\n",
    "        en, ru = [batch[key] for key in [\"en\", \"ru\"]]\n",
    "        src = self.encode(en, ...)\n",
    "        dst = self.encode(ru, ...)\n",
    "        <your code if needed>\n",
    "        src = torch.LongTensor(src)\n",
    "        dst = torch.LongTensor(dst)\n",
    "        with torch.no_grad():\n",
    "            # generate ouput autoregressively\n",
    "            for i in range(10):  # MAX_LEN - 1\n",
    "                <your code>\n",
    "            dst = dst.cpu().numpy()\n",
    "            dst = [line.tolist() for line in dst]\n",
    "            dst = self.bpe.decode(dst)\n",
    "            dst = [line.replace(\"<PAD>\", \"\") for line in dst]\n",
    "        for line in zip(en, ru, dst):\n",
    "            print(\"\\t\".join(line))\n",
    "    \n",
    "    def compute_all(self, batch):\n",
    "        en, ru = [batch[key] for key in [\"en\", \"ru\"]]\n",
    "        <formulate task>\n",
    "        src = self.encode(en, ...)\n",
    "        dst = self.encode(ru, ... )\n",
    "        \n",
    "        src = torch.LongTensor(src)\n",
    "        dst = torch.LongTensor(dst)\n",
    "\n",
    "        output = self.forward(src, dst)\n",
    "        \n",
    "        <compute loss>\n",
    "        loss = ...\n",
    "        \n",
    "        return dict(\n",
    "            loss=loss,\n",
    "            metrics=dict(\n",
    "                loss=loss.item(),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, src, dst):\n",
    "        # let's a little hack:\n",
    "        device = next(self.parameters()).device\n",
    "        src = src.to(device)\n",
    "        dst = dst.to(device)\n",
    "        \n",
    "        \n",
    "        <build embeddings for tokens and positional>\n",
    "        \n",
    "        # embedded = embedded_tokens * sqrt(hidden_size) + embedded_positions\n",
    "        \n",
    "        <reshape properly>\n",
    "        \n",
    "        <build pad masks>\n",
    "        src_pad_mask = src != 0\n",
    "        dst_pad_mask = dst != 0\n",
    "        \n",
    "        output = self.transformer(src_embedded, dst_embedded, \n",
    "                                  src_key_padding_mask=src_pad_mask, \n",
    "                                  tgt_key_padding_mask=dst_pad_mask)\n",
    "        <predict next token probs>\n",
    "        <permute to [bs, vocab, seq_len]> \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем, размерности\n",
    "model = VeryT(bpe)\n",
    "with torch.no_grad():\n",
    "    batch = next(it)\n",
    "    model.check_translations(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=\"cuda:0\"\n",
    "print(device)\n",
    "\n",
    "model = VeryT(bpe)\n",
    "model.to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=50, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=50, shuffle=False)\n",
    "\n",
    "train_model(model, opt, trainloader, valloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
